#ifndef MONONN_H
#define MONONN_H

#include <mononn.hpp>

#endif

/*


## Theorem Sketch

### **Universal Approximation (generated by Grok)**

- Monomial networks with fixed $ m \geq 2 $ and sufficient layers/neurons can approximate any continuous function on compact sets. Why?
  - Powers `x^m` span nonlinear basis
  - Layer composition generates dense function class
  - More expressive than linear MLPs for same width/depth

## Proof of Universal Approximation

**Theorem**: Monomial Neural Networks with odd degree $n$ (or even $n$ on positive inputs) are universal approximators.

**Proof**:

1.  **Equivalence to Standard MLP**:
    Let the operation of layer $k$ be $h_k = \sigma(C_k (h_{k-1})^n + B_k)$.
    This can be rewritten as $h_k = \sigma(C_k \tilde{h}_{k-1} + B_k)$ where $\tilde{h}_{k-1} = (h_{k-1})^n$.
    Alternatively, consider the effective activation function of the previous layer. If the output of layer $k-1$ is $h_{k-1} = \sigma(z_{k-1})$, then the input to the linear part of layer $k$ is $(\sigma(z_{k-1}))^n$.
    Define a new activation function $\tilde{\sigma}(z) = (\sigma(z))^n$.
    The network is then mathematically equivalent to a standard Multi-Layer Perceptron (MLP) using the activation function $\tilde{\sigma}$, with an initial input transformation $\phi(x) = x^n$.

2.  **Applicability of UAT**:
    Standard Universal Approximation Theorems (e.g., Hornik 1991) state that an MLP with a non-polynomial activation function can approximate any continuous function on compact sets.

    - If $\sigma$ is Sigmoid, $\tilde{\sigma}(z) = (1/(1+e^{-z}))^n$ is sigmoidal and non-polynomial.
    - If $\sigma$ is ReLU, $\tilde{\sigma}(z) = \max(0, z)^n$ is non-polynomial (0 for $z<0$).
      Thus, the core network structure is a universal approximator with respect to the transformed input $u = x^n$.

3.  **Input Mapping**:
    - If $n$ is **odd**, the mapping $u = x^n$ is a homeomorphism (bijective and continuous with continuous inverse) on $\mathbb{R}$. Approximating $f(x)$ is equivalent to approximating $f(u^{1/n})$, which is continuous.
    - If $n$ is **even**, $u = x^n$ is symmetric. The network can universally approximate any continuous **even** function (or any function on $\mathbb{R}_{\ge 0}$).

**Conclusion**: The Monomial Neural Network preserves the universal approximation capabilities of standard MLPs, subject to the symmetry constraints of the monomial degree $n$.


**Theorem**: A Monomial Neural Network (MNN) with monomial degree $n$ (where $n$ is an odd integer) and a suitable non-linear activation function $\sigma$ (e.g., Sigmoid, Softmax) can approximate any continuous function $f: K \to \mathbb{R}^m$ on a compact set $K \subset \mathbb{R}^d$ to arbitrary accuracy.

**Proof**:

**Step 1: Mathematical Formulation of a Monomial Layer**
Let $x \in \mathbb{R}^{d_{in}}$ be the input to a layer. A standard neuron computes $z = \sum w_i x_i + b$.
A monomial neuron computes the pre-activation value $z$ as:
$$ z*j = \sum*{i=1}^{d*{in}} w*{ji} (x_i)^n + b_j $$
The output of the layer is $h = \sigma(z)$.

**Step 2: Input Transformation and Equivalence**
Define a transformation map $\Phi: \mathbb{R}^d \to \mathbb{R}^d$ such that:
$$ u = \Phi(x) = x^n $$
(applied element-wise).
Substituting $u$ into the monomial equation:
$$ z*j = \sum*{i=1}^{d*{in}} w*{ji} u_i + b_j $$
This equation represents a standard linear layer acting on the transformed input $u$.
Therefore, an MNN with input $x$ is mathematically equivalent to a standard Multi-Layer Perceptron (MLP) with input $u = x^n$.
$$ \text{MNN}(x) \equiv \text{MLP}(x^n) $$

**Step 3: Universal Approximation of the MLP**
By the Universal Approximation Theorem (Hornik et al., 1989; Cybenko, 1989), a standard MLP with a continuous, bounded, and non-constant activation function $\sigma$ can approximate any continuous function $g(u)$ on a compact set $U$.

- **Sigmoid**: $\sigma(z) = \frac{1}{1+e^{-z}}$ is continuous, bounded $(0, 1)$, and non-constant.
- **Softmax**: $\sigma(z)_i = \frac{e^{z_i}}{\sum e^{z_k}}$ is continuous and bounded $[0, 1]$. While vector-valued, it provides the necessary non-linearity and density properties for approximation when combined with linear layers.

Thus, there exists an MLP such that:
$$ |\text{MLP}(u) - g(u)| < \epsilon $$

**Step 4: Mapping to the Target Function**
We wish to approximate a target function $f(x)$.
We define the function $g(u)$ such that $g(u) = f(x)$.
For this to be a valid function definition, the mapping from $u$ to $x$ must be unique.

- Since $n$ is **odd**, the function $u = x^n$ is bijective (one-to-one and onto) for $x \in \mathbb{R}$.
- The inverse mapping $x = u^{1/n}$ is well-defined and continuous.
- Therefore, we can define $g(u) = f(u^{1/n})$.

Since $f$ is continuous and the inverse mapping is continuous, the composition $g(u)$ is also continuous.

**Step 5: Conclusion**
Since $g(u)$ is continuous, the MLP can approximate it:
$$ |\text{MLP}(u) - f(u^{1/n})| < \epsilon $$
Substituting $u = x^n$ back into the equation:
$$ |\text{MLP}(x^n) - f(x)| < \epsilon $$
Since $\text{MNN}(x) = \text{MLP}(x^n)$, we have:
$$ |\text{MNN}(x) - f(x)| < \epsilon $$

**Q.E.D.**

_Note: If $n$ is even, the approximation holds for the subspace of even functions ($f(x) = f(-x)$) or for inputs restricted to the positive domain ($x \ge 0$)._

*/